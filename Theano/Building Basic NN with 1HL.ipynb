{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 1-hidden-layer neural network in Theano.\n",
    "# This code is not optimized for speed.\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "# from util import get_normalized_data, y2indicator\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_normalized_data():\n",
    "    print(\"Reading in and transforming data...\")\n",
    "\n",
    "    df = pd.read_csv('./large_files/train.csv')\n",
    "    data = df.as_matrix().astype(np.float32)\n",
    "    np.random.shuffle(data)\n",
    "    X = data[:, 1:]\n",
    "    mu = X.mean(axis=0)\n",
    "    std = X.std(axis=0)\n",
    "    np.place(std, std == 0, 1)\n",
    "    X = (X - mu) / std # normalize the data\n",
    "    Y = data[:, 0]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def y2indicator(y):\n",
    "    N = len(y)\n",
    "    y = y.astype(np.int32)\n",
    "    ind = np.zeros((N, 10))\n",
    "    for i in range(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(p, t):\n",
    "    return np.mean(p != t)\n",
    "\n",
    "def relu(a):\n",
    "    return a * (a > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in and transforming data...\n"
     ]
    }
   ],
   "source": [
    "# step 1: get the data and define all the usual variables\n",
    "X, Y = get_normalized_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_iter = 20\n",
    "print_period = 10\n",
    "\n",
    "lr = 0.00004\n",
    "reg = 0.01\n",
    "\n",
    "Xtrain = X[:-1000,]\n",
    "Ytrain = Y[:-1000]\n",
    "Xtest  = X[-1000:,]\n",
    "Ytest  = Y[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ytrain_ind = y2indicator(Ytrain)\n",
    "Ytest_ind = y2indicator(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N, D = Xtrain.shape\n",
    "batch_sz = 500\n",
    "n_batches = N // batch_sz\n",
    "\n",
    "M = 300\n",
    "K = 10\n",
    "W1_init = np.random.randn(D, M) / 28\n",
    "b1_init = np.zeros(M)\n",
    "W2_init = np.random.randn(M, K) / np.sqrt(M)\n",
    "b2_init = np.zeros(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.random.randn(D, M) / 28)\n",
    "np.shape(np.random.randn(M, K) / np.sqrt(M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: define theano variables and expressions\n",
    "thX = T.matrix('X')\n",
    "thT = T.matrix('T')\n",
    "W1 = theano.shared(W1_init, 'W1')\n",
    "b1 = theano.shared(b1_init, 'b1')\n",
    "W2 = theano.shared(W2_init, 'W2')\n",
    "b2 = theano.shared(b2_init, 'b2')\n",
    "\n",
    "# we can use the built-in theano functions to do relu and softmax\n",
    "thZ = relu( thX.dot(W1) + b1 ) # relu is new in version 0.7.1 but just in case you don't have it\n",
    "thY = T.nnet.softmax( thZ.dot(W2) + b2 )\n",
    "\n",
    "# define the cost function and prediction\n",
    "cost = -(thT * T.log(thY)).sum() + reg*((W1*W1).sum() + (b1*b1).sum() + (W2*W2).sum() + (b2*b2).sum())\n",
    "prediction = T.argmax(thY, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost / err at iteration i=0, j=0: 2380.889 / 0.854\n",
      "Cost / err at iteration i=0, j=10: 1797.146 / 0.502\n",
      "Cost / err at iteration i=0, j=20: 1437.243 / 0.362\n",
      "Cost / err at iteration i=0, j=30: 1197.259 / 0.265\n",
      "Cost / err at iteration i=0, j=40: 1028.302 / 0.213\n",
      "Cost / err at iteration i=0, j=50: 905.242 / 0.182\n",
      "Cost / err at iteration i=0, j=60: 813.534 / 0.172\n",
      "Cost / err at iteration i=0, j=70: 743.273 / 0.156\n",
      "Cost / err at iteration i=0, j=80: 685.248 / 0.150\n",
      "Cost / err at iteration i=1, j=0: 674.329 / 0.144\n",
      "Cost / err at iteration i=1, j=10: 630.691 / 0.141\n",
      "Cost / err at iteration i=1, j=20: 594.920 / 0.134\n",
      "Cost / err at iteration i=1, j=30: 564.808 / 0.133\n",
      "Cost / err at iteration i=1, j=40: 536.480 / 0.123\n",
      "Cost / err at iteration i=1, j=50: 513.407 / 0.123\n",
      "Cost / err at iteration i=1, j=60: 493.628 / 0.117\n",
      "Cost / err at iteration i=1, j=70: 477.180 / 0.113\n",
      "Cost / err at iteration i=1, j=80: 460.479 / 0.115\n",
      "Cost / err at iteration i=2, j=0: 457.289 / 0.114\n",
      "Cost / err at iteration i=2, j=10: 444.010 / 0.110\n",
      "Cost / err at iteration i=2, j=20: 432.412 / 0.109\n",
      "Cost / err at iteration i=2, j=30: 422.208 / 0.107\n",
      "Cost / err at iteration i=2, j=40: 410.666 / 0.105\n",
      "Cost / err at iteration i=2, j=50: 401.610 / 0.105\n",
      "Cost / err at iteration i=2, j=60: 393.548 / 0.102\n",
      "Cost / err at iteration i=2, j=70: 386.612 / 0.104\n",
      "Cost / err at iteration i=2, j=80: 378.287 / 0.099\n",
      "Cost / err at iteration i=3, j=0: 376.727 / 0.097\n",
      "Cost / err at iteration i=3, j=10: 370.153 / 0.096\n",
      "Cost / err at iteration i=3, j=20: 364.508 / 0.098\n",
      "Cost / err at iteration i=3, j=30: 359.385 / 0.097\n",
      "Cost / err at iteration i=3, j=40: 352.822 / 0.095\n",
      "Cost / err at iteration i=3, j=50: 348.133 / 0.095\n",
      "Cost / err at iteration i=3, j=60: 343.852 / 0.095\n",
      "Cost / err at iteration i=3, j=70: 340.061 / 0.095\n",
      "Cost / err at iteration i=3, j=80: 334.719 / 0.093\n",
      "Cost / err at iteration i=4, j=0: 333.703 / 0.092\n",
      "Cost / err at iteration i=4, j=10: 329.618 / 0.090\n",
      "Cost / err at iteration i=4, j=20: 326.244 / 0.090\n",
      "Cost / err at iteration i=4, j=30: 323.004 / 0.087\n",
      "Cost / err at iteration i=4, j=40: 318.559 / 0.085\n",
      "Cost / err at iteration i=4, j=50: 315.697 / 0.086\n",
      "Cost / err at iteration i=4, j=60: 312.967 / 0.084\n",
      "Cost / err at iteration i=4, j=70: 310.585 / 0.087\n",
      "Cost / err at iteration i=4, j=80: 306.713 / 0.085\n",
      "Cost / err at iteration i=5, j=0: 305.941 / 0.084\n",
      "Cost / err at iteration i=5, j=10: 303.023 / 0.085\n",
      "Cost / err at iteration i=5, j=20: 300.753 / 0.086\n",
      "Cost / err at iteration i=5, j=30: 298.483 / 0.084\n",
      "Cost / err at iteration i=5, j=40: 295.147 / 0.082\n",
      "Cost / err at iteration i=5, j=50: 293.235 / 0.083\n",
      "Cost / err at iteration i=5, j=60: 291.289 / 0.083\n",
      "Cost / err at iteration i=5, j=70: 289.701 / 0.086\n",
      "Cost / err at iteration i=5, j=80: 286.695 / 0.083\n",
      "Cost / err at iteration i=6, j=0: 286.060 / 0.082\n",
      "Cost / err at iteration i=6, j=10: 283.800 / 0.082\n",
      "Cost / err at iteration i=6, j=20: 282.150 / 0.084\n",
      "Cost / err at iteration i=6, j=30: 280.463 / 0.083\n",
      "Cost / err at iteration i=6, j=40: 277.798 / 0.081\n",
      "Cost / err at iteration i=6, j=50: 276.458 / 0.081\n",
      "Cost / err at iteration i=6, j=60: 275.008 / 0.081\n",
      "Cost / err at iteration i=6, j=70: 273.882 / 0.083\n",
      "Cost / err at iteration i=6, j=80: 271.398 / 0.083\n",
      "Cost / err at iteration i=7, j=0: 270.846 / 0.081\n",
      "Cost / err at iteration i=7, j=10: 268.957 / 0.082\n",
      "Cost / err at iteration i=7, j=20: 267.655 / 0.081\n",
      "Cost / err at iteration i=7, j=30: 266.266 / 0.081\n",
      "Cost / err at iteration i=7, j=40: 264.017 / 0.079\n",
      "Cost / err at iteration i=7, j=50: 263.052 / 0.079\n",
      "Cost / err at iteration i=7, j=60: 261.920 / 0.080\n",
      "Cost / err at iteration i=7, j=70: 261.085 / 0.079\n",
      "Cost / err at iteration i=7, j=80: 258.963 / 0.079\n",
      "Cost / err at iteration i=8, j=0: 258.467 / 0.079\n",
      "Cost / err at iteration i=8, j=10: 256.831 / 0.078\n",
      "Cost / err at iteration i=8, j=20: 255.761 / 0.075\n",
      "Cost / err at iteration i=8, j=30: 254.531 / 0.077\n",
      "Cost / err at iteration i=8, j=40: 252.605 / 0.076\n",
      "Cost / err at iteration i=8, j=50: 251.903 / 0.074\n",
      "Cost / err at iteration i=8, j=60: 251.010 / 0.076\n",
      "Cost / err at iteration i=8, j=70: 250.377 / 0.075\n",
      "Cost / err at iteration i=8, j=80: 248.525 / 0.074\n",
      "Cost / err at iteration i=9, j=0: 248.070 / 0.074\n",
      "Cost / err at iteration i=9, j=10: 246.646 / 0.074\n",
      "Cost / err at iteration i=9, j=20: 245.752 / 0.072\n",
      "Cost / err at iteration i=9, j=30: 244.672 / 0.072\n",
      "Cost / err at iteration i=9, j=40: 242.977 / 0.072\n",
      "Cost / err at iteration i=9, j=50: 242.474 / 0.072\n",
      "Cost / err at iteration i=9, j=60: 241.746 / 0.071\n",
      "Cost / err at iteration i=9, j=70: 241.233 / 0.072\n",
      "Cost / err at iteration i=9, j=80: 239.593 / 0.072\n",
      "Cost / err at iteration i=10, j=0: 239.181 / 0.071\n",
      "Cost / err at iteration i=10, j=10: 237.931 / 0.073\n",
      "Cost / err at iteration i=10, j=20: 237.171 / 0.072\n",
      "Cost / err at iteration i=10, j=30: 236.201 / 0.072\n",
      "Cost / err at iteration i=10, j=40: 234.665 / 0.070\n",
      "Cost / err at iteration i=10, j=50: 234.287 / 0.069\n",
      "Cost / err at iteration i=10, j=60: 233.691 / 0.068\n",
      "Cost / err at iteration i=10, j=70: 233.283 / 0.070\n",
      "Cost / err at iteration i=10, j=80: 231.799 / 0.069\n",
      "Cost / err at iteration i=11, j=0: 231.415 / 0.070\n",
      "Cost / err at iteration i=11, j=10: 230.290 / 0.071\n",
      "Cost / err at iteration i=11, j=20: 229.628 / 0.070\n",
      "Cost / err at iteration i=11, j=30: 228.755 / 0.069\n",
      "Cost / err at iteration i=11, j=40: 227.362 / 0.069\n",
      "Cost / err at iteration i=11, j=50: 227.090 / 0.068\n",
      "Cost / err at iteration i=11, j=60: 226.604 / 0.067\n",
      "Cost / err at iteration i=11, j=70: 226.273 / 0.068\n",
      "Cost / err at iteration i=11, j=80: 224.916 / 0.069\n",
      "Cost / err at iteration i=12, j=0: 224.561 / 0.069\n",
      "Cost / err at iteration i=12, j=10: 223.543 / 0.068\n",
      "Cost / err at iteration i=12, j=20: 222.955 / 0.067\n",
      "Cost / err at iteration i=12, j=30: 222.164 / 0.067\n",
      "Cost / err at iteration i=12, j=40: 220.869 / 0.068\n",
      "Cost / err at iteration i=12, j=50: 220.685 / 0.066\n",
      "Cost / err at iteration i=12, j=60: 220.288 / 0.065\n",
      "Cost / err at iteration i=12, j=70: 220.008 / 0.067\n",
      "Cost / err at iteration i=12, j=80: 218.743 / 0.067\n",
      "Cost / err at iteration i=13, j=0: 218.419 / 0.066\n",
      "Cost / err at iteration i=13, j=10: 217.481 / 0.065\n",
      "Cost / err at iteration i=13, j=20: 216.958 / 0.066\n",
      "Cost / err at iteration i=13, j=30: 216.242 / 0.065\n",
      "Cost / err at iteration i=13, j=40: 215.026 / 0.064\n",
      "Cost / err at iteration i=13, j=50: 214.912 / 0.064\n",
      "Cost / err at iteration i=13, j=60: 214.580 / 0.063\n",
      "Cost / err at iteration i=13, j=70: 214.353 / 0.064\n",
      "Cost / err at iteration i=13, j=80: 213.172 / 0.065\n",
      "Cost / err at iteration i=14, j=0: 212.862 / 0.064\n",
      "Cost / err at iteration i=14, j=10: 212.002 / 0.062\n",
      "Cost / err at iteration i=14, j=20: 211.529 / 0.064\n",
      "Cost / err at iteration i=14, j=30: 210.871 / 0.062\n",
      "Cost / err at iteration i=14, j=40: 209.711 / 0.061\n",
      "Cost / err at iteration i=14, j=50: 209.664 / 0.062\n",
      "Cost / err at iteration i=14, j=60: 209.401 / 0.060\n",
      "Cost / err at iteration i=14, j=70: 209.218 / 0.060\n",
      "Cost / err at iteration i=14, j=80: 208.103 / 0.061\n",
      "Cost / err at iteration i=15, j=0: 207.809 / 0.061\n",
      "Cost / err at iteration i=15, j=10: 207.010 / 0.061\n",
      "Cost / err at iteration i=15, j=20: 206.578 / 0.060\n",
      "Cost / err at iteration i=15, j=30: 205.966 / 0.061\n",
      "Cost / err at iteration i=15, j=40: 204.859 / 0.061\n",
      "Cost / err at iteration i=15, j=50: 204.861 / 0.060\n",
      "Cost / err at iteration i=15, j=60: 204.648 / 0.060\n",
      "Cost / err at iteration i=15, j=70: 204.503 / 0.060\n",
      "Cost / err at iteration i=15, j=80: 203.455 / 0.061\n",
      "Cost / err at iteration i=16, j=0: 203.175 / 0.061\n",
      "Cost / err at iteration i=16, j=10: 202.429 / 0.060\n",
      "Cost / err at iteration i=16, j=20: 202.039 / 0.060\n",
      "Cost / err at iteration i=16, j=30: 201.474 / 0.060\n",
      "Cost / err at iteration i=16, j=40: 200.409 / 0.060\n",
      "Cost / err at iteration i=16, j=50: 200.452 / 0.060\n",
      "Cost / err at iteration i=16, j=60: 200.284 / 0.060\n",
      "Cost / err at iteration i=16, j=70: 200.156 / 0.060\n",
      "Cost / err at iteration i=16, j=80: 199.164 / 0.061\n",
      "Cost / err at iteration i=17, j=0: 198.900 / 0.061\n",
      "Cost / err at iteration i=17, j=10: 198.195 / 0.060\n",
      "Cost / err at iteration i=17, j=20: 197.830 / 0.060\n",
      "Cost / err at iteration i=17, j=30: 197.302 / 0.060\n",
      "Cost / err at iteration i=17, j=40: 196.278 / 0.060\n",
      "Cost / err at iteration i=17, j=50: 196.347 / 0.060\n",
      "Cost / err at iteration i=17, j=60: 196.227 / 0.059\n",
      "Cost / err at iteration i=17, j=70: 196.123 / 0.059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost / err at iteration i=17, j=80: 195.175 / 0.061\n",
      "Cost / err at iteration i=18, j=0: 194.933 / 0.061\n",
      "Cost / err at iteration i=18, j=10: 194.259 / 0.061\n",
      "Cost / err at iteration i=18, j=20: 193.917 / 0.060\n",
      "Cost / err at iteration i=18, j=30: 193.422 / 0.059\n",
      "Cost / err at iteration i=18, j=40: 192.426 / 0.058\n",
      "Cost / err at iteration i=18, j=50: 192.516 / 0.057\n",
      "Cost / err at iteration i=18, j=60: 192.438 / 0.058\n",
      "Cost / err at iteration i=18, j=70: 192.347 / 0.057\n",
      "Cost / err at iteration i=18, j=80: 191.444 / 0.060\n",
      "Cost / err at iteration i=19, j=0: 191.213 / 0.060\n",
      "Cost / err at iteration i=19, j=10: 190.581 / 0.060\n",
      "Cost / err at iteration i=19, j=20: 190.260 / 0.058\n",
      "Cost / err at iteration i=19, j=30: 189.797 / 0.058\n",
      "Cost / err at iteration i=19, j=40: 188.833 / 0.059\n",
      "Cost / err at iteration i=19, j=50: 188.944 / 0.057\n",
      "Cost / err at iteration i=19, j=60: 188.899 / 0.059\n",
      "Cost / err at iteration i=19, j=70: 188.830 / 0.057\n",
      "Cost / err at iteration i=19, j=80: 187.969 / 0.058\n"
     ]
    }
   ],
   "source": [
    "# step 3: training expressions and functions\n",
    "# we can just include regularization as part of the cost because it is also automatically differentiated!\n",
    "# update_W1 = W1 - lr*(T.grad(cost, W1) + reg*W1)\n",
    "# update_b1 = b1 - lr*(T.grad(cost, b1) + reg*b1)\n",
    "# update_W2 = W2 - lr*(T.grad(cost, W2) + reg*W2)\n",
    "# update_b2 = b2 - lr*(T.grad(cost, b2) + reg*b2)\n",
    "update_W1 = W1 - lr*T.grad(cost, W1)\n",
    "update_b1 = b1 - lr*T.grad(cost, b1)\n",
    "update_W2 = W2 - lr*T.grad(cost, W2)\n",
    "update_b2 = b2 - lr*T.grad(cost, b2)\n",
    "\n",
    "train = theano.function(\n",
    "    inputs=[thX, thT],\n",
    "    updates=[(W1, update_W1), (b1, update_b1), (W2, update_W2), (b2, update_b2)],\n",
    ")\n",
    "\n",
    "# create another function for this because we want it over the whole dataset\n",
    "get_prediction = theano.function(\n",
    "    inputs=[thX, thT],\n",
    "    outputs=[cost, prediction],\n",
    ")\n",
    "\n",
    "costs = []\n",
    "for i in range(max_iter):\n",
    "    for j in range(n_batches):\n",
    "        Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "        Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "\n",
    "        train(Xbatch, Ybatch)\n",
    "        if j % print_period == 0:\n",
    "            cost_val, prediction_val = get_prediction(Xtest, Ytest_ind)\n",
    "            err = error_rate(prediction_val, Ytest)\n",
    "            print(\"Cost / err at iteration i=%d, j=%d: %.3f / %.3f\" % (i, j, cost_val, err))\n",
    "            costs.append(cost_val)\n",
    "# how would you incorporate momentum into the gradient descent procedure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKIUACIiEaUAEBGJqP8PkxquJIaHnNMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c18d933c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
